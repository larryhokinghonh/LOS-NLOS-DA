{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression Pipeline\n",
    "\n",
    "This notebook walks through a full machine learning pipeline for predicting the `RANGE` variable using **Gradient Boosting Regressor**. The pipeline includes:\n",
    "- Loading and preparing the dataset\n",
    "- Feature scaling\n",
    "- Hyperparameter tuning using Randomized Search with Cross-Validation\n",
    "- Evaluating model performance with metrics such as MSE, RMSE, MAE, and R²\n",
    "- Analyzing feature importance\n",
    "- Visualizing residuals and saving the final model for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Dataset\n",
    "\n",
    "We begin by loading the preprocessed dataset using `pandas.read_csv`. This dataset contains features and a target variable named `RANGE`. After loading, we print the shape of the dataset to confirm its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv('../data/processed/aggregated_dataset.csv')\n",
    "print(\"Dataset shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Features and Target\n",
    "\n",
    "We drop the `RANGE` (target) and `NLOS` (non-line-of-sight flag) columns from the dataset to form the feature matrix `X`. The `RANGE` column is stored separately as the target variable `y`.\n",
    "\n",
    "We also split the data into training and test sets using `train_test_split`, with 80% used for training and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop(['RANGE', 'NLOS'], axis=1)  # Remove both RANGE and NLOS\n",
    "y = data['RANGE']  # Target is RANGE\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Features with RobustScaler\n",
    "\n",
    "We use `RobustScaler` to scale the input features. This scaler is robust to outliers, as it uses the median and interquartile range rather than mean and standard deviation. We fit it to the training data and apply the same transformation to the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Gradient Boosting Regressor and Define Hyperparameter Space\n",
    "\n",
    "We initialize a `GradientBoostingRegressor` with a fixed `random_state` for reproducibility.\n",
    "\n",
    "Then, we define a dictionary of hyperparameters (`param_distributions`) to explore. In this case, we're specifying fixed values for each key hyperparameter to evaluate via randomized search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base model\n",
    "rf_reg = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "\n",
    "# Random search for hyperparameter tuning\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_reg,\n",
    "    # Define hyperparameter search space\n",
    "    param_distributions={\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': ['sqrt']\n",
    "},\n",
    "    n_iter=1,\n",
    "    cv=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Training Data\n",
    "\n",
    "The model is now fit to the training data (`X_train_scaled`, `y_train`). The best hyperparameters and the corresponding MSE score from cross-validation are printed after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search model\n",
    "rf_random.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"\\nBest parameters:\", rf_random.best_params_)\n",
    "print(\"Best MSE score:\", -rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Best Estimator\n",
    "\n",
    "Once training and tuning are complete, we extract the best-performing model (based on CV MSE score) for further evaluation and predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_rf_reg = rf_random.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_rf_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Range')\n",
    "plt.ylabel('Predicted Range')\n",
    "plt.title('Actual vs Predicted Range Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Top 20 Feature Importances\n",
    "\n",
    "We extract and plot the top 20 most important features according to the trained Gradient Boosting model. This helps interpret which input variables have the most influence on predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_rf_reg.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Top 20 Most Important Features for Range Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Model and Scaler\n",
    "\n",
    "The trained model (`best_rf_reg`) and the `RobustScaler` used for feature scaling are saved using `joblib`. This allows the model to be reused later without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scaler\n",
    "joblib.dump(best_rf_reg, '../models/best_rf_regressor.joblib')\n",
    "joblib.dump(scaler, '../models/rf_regressor_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis\n",
    "\n",
    "Residuals (actual - predicted values) give insight into the prediction errors. A scatter plot of residuals helps identify patterns such as bias or non-random error distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Range')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Distribution of Residuals\n",
    "\n",
    "A histogram shows the distribution of residuals. Ideally, residuals should be roughly normally distributed around 0, indicating unbiased predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.xlabel('Residual Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics of Residuals\n",
    "\n",
    "We display descriptive statistics (mean, std, quartiles) for the residuals to better understand the spread and central tendency of prediction errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics of residuals\n",
    "print(\"\\nResiduals Summary Statistics:\")\n",
    "print(pd.Series(residuals).describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
